{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae40659",
   "metadata": {},
   "source": [
    "# Web scraping of a Gaming Website using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c30bdd",
   "metadata": {},
   "source": [
    "Coming from the days where nearly no one knew much about the gaming news to having the websites onboard that deliver even the minute updates about games. Gamers are always in search of the latest release and updates of games. To make this information available in a structured way, the objective of this project is to extract the information of one such gaming website - [DigiStatement](https://digistatement.com/).\n",
    "![digi-statement](https://www.linkpicture.com/q/Screenshot_20221226_194400.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95416701",
   "metadata": {},
   "source": [
    "## Introduction to [Web Scrapping](https://www.geeksforgeeks.org/what-is-web-scraping-and-how-to-use-it/)\n",
    "<b> What is meant by web scraping?</b> <br>\n",
    "![image-4.png](https://i.ibb.co/2PdK54T/3.png)\n",
    "Web scraping is the process of extracting the unstrutured information out of the webpage and convert it into a structured data using spreadsheets or any database. Many webscraping methods are used to scrape the data. For example, [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) is a python package that helps in webscraping.\n",
    "Since the websites have so much data coming in from all around the places. The main frame of the website is the same but the data it shows change over time. Web scrapping is a way to collect the required data for some personal and professional use.\n",
    "It is a way to collect the data in a structured form. The project is done using the [Python Programming Language](https://en.wikipedia.org/wiki/Python_(programming_language))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777717bc",
   "metadata": {},
   "source": [
    "\n",
    "## Outline\n",
    "Listed are the steps that we will be using to build the project we aim:\n",
    "1. Installing the required inbuilt python libraries and functions such as `requests` and `beautifulsoup4`.\n",
    "2. Parse the HTML source code using beautiful soup\n",
    "3. Extract blog Names, Author Name, Date and URLs from page\n",
    "4. Extract and combine data from multiple pages\n",
    "5. Compile extracted information into Python lists and dictionaries\n",
    "6. Save the extracted information to a CSV file\n",
    "\n",
    "By the end of this project, we will be able to see the `Top 100 Blogs on DigiStatement` in the following `csv` file format:\n",
    "![final csv](https://www.linkpicture.com/q/Screenshot_20221227_111300.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ad2a4",
   "metadata": {},
   "source": [
    "## 1.  How to get started with the project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ba1d3b",
   "metadata": {},
   "source": [
    "This project is hosted on Jovian. To initialise making of the project, we need to use the `Jupyter Notebook` and run the jupyter notebook on `Binder`. Other alternative is to use `Anaconda`.\n",
    "![run](https://i.imgur.com/J6vTodm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651c7e7",
   "metadata": {},
   "source": [
    "## 2. Installing and importing the Libraries required\n",
    "We will proceed by importing the libraries which will be used later in the project\n",
    "* <b>Requests </b> - to download the the web page in the text format.\n",
    "* <b>BeautifulSoup</b> - to be able to use the text downloaded from the Requests library to be used for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8856876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --quiet command is used to hide the processing that goes into installing the libraries.\n",
    "# --upgrade is used to install the latest version available (if any).\n",
    "!pip install requests --upgrade --quiet\n",
    "!pip install beautifulsoup4 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be81deac",
   "metadata": {},
   "source": [
    "Importing the `modules` that we require from the installed `libraries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d48353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efede3",
   "metadata": {},
   "source": [
    "## 3. Creating the function `get_page()` \n",
    "1. The base url is stated and it directs the function to the webpage. \n",
    "2. This function takes `page number` as the argument to target the required web pages.\n",
    "3. `page_url` variable stores the complete path of the webpage to be extracted, including the argument. \n",
    "4. `requests.get` is used to render the url passed.\n",
    "5. Status code is verified using the `if` statement. It must be between 200-299 for successfully extracting the information out. Most of the times, it comes out as 200 only.\n",
    "6. Finally, a variable `doc` is created which renders the webpage to be extracted using `BeautifulSoup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fe8b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(page_number):\n",
    "    base_url= \"https://digistatement.com/page/\"\n",
    "    page_url = base_url + str(page_number) + \"/\"\n",
    "    response = requests.get(page_url)\n",
    "    response.status_code\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"Page did not scrape correctly.\")\n",
    "    \n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4088789",
   "metadata": {},
   "source": [
    "## 4. Creating the function `parse_blog()` to extract Blog Title, Author's Name, URL, and Date of Publishing of a particular page\n",
    "1. It takes the argument as `doc` that was extracted in the previous function.\n",
    "2. The `a_tags` variable stores the `a` tags found through `find_all()`, present under the H3 tags of a particular article tag. It has been extracted because the `a` tags present in H3 tags stores the title and url information that can be extracted. The same is stored in the `title` and `url` variables.\n",
    "3. Similarly, for extracting the Date and Author's Name, `div` tags of the respective classes are drawn out.\n",
    "4. Finally, a dictionary is returned showing Title, Author's Name, URL, and Date of Publishing with the right information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6c26ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_blog (doc):\n",
    "    a_tags = doc.h3.find_all('a')\n",
    "    title = a_tags[0].text.strip()\n",
    "    url = a_tags[0]['href']\n",
    "    author_tag = doc.find_all('div',class_=\"jeg_meta_author\")\n",
    "    author_name = author_tag[0].text.strip().replace('by ','')\n",
    "    date_tag = doc.find_all('div',class_=\"jeg_meta_date\")\n",
    "    date = date_tag[0].text.strip()\n",
    "    return {\n",
    "        'Blog Title':title,\n",
    "        'Author Name': author_name,\n",
    "        'URL': url,\n",
    "        'Date of Publishing': date\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6429c",
   "metadata": {},
   "source": [
    "## 5. Creating the function `get_top_blogs()`\n",
    "1. It takes the argument as `doc` which was the return value of an earlier function.\n",
    "2. Each result present on web pages of DigiStatement is stored under `<article>` tags. Therefore, `article_tags` variable stores all the article tags extracted from a particular page. \n",
    "3. Finally, a list of all dictionaries is returned consisting of Title, Author's Name, URL, and Date of Publishing for each `article` tag present on a web page using a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d52bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_blogs(doc):\n",
    "    article_tags = doc.find_all('article', class_ = \"jeg_post jeg_pl_md_2 format-standard\" )\n",
    "    blogs_on_page = [parse_blog(i) for i in article_tags]\n",
    "    return blogs_on_page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d5e30",
   "metadata": {},
   "source": [
    "## 6. Extracting top 100 Blogs of DigiStatement from its first 10 pages.\n",
    "\n",
    "As mentioned earlier, each page on DigiStatement returns 10 blogs sorted by the latest date. To extract top 100 blogs of this website, we need to firstly, extract first 10 pages using the `get_page(page_number)` function and then applying the `get_top_blogs(doc)` function to extract the information of 10 blogs present at each page. All of this has been done using an empty list and `for` loop.\n",
    "\n",
    "`get_top100_blogs()` function is defined to do everything mentioned above and returning the required 100 results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f405e15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top100_blogs():\n",
    "    top_100 = []\n",
    "    for i in range(1,11):\n",
    "        top_100.append(get_top_blogs(get_page(i)))\n",
    "    return top_100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573601c3",
   "metadata": {},
   "source": [
    "## 7. Creating a flat list to get a clean result\n",
    "\n",
    "The `get_top100_blogs()` function returns a list of list that cannot be written to a csv file properly. Therefore, the following function, `flatten_list(2d_list)` has been created to create a single list out of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f358d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    for e in _2d_list:\n",
    "        if type(e) is list:\n",
    "            for i in e:\n",
    "                flat_list.append(i)\n",
    "        else:\n",
    "            flat_list.append(e)\n",
    "    return flat_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb7b81b",
   "metadata": {},
   "source": [
    "## 8. Creating a function to deploy the final result into a \".csv\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02163d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv (items, path):\n",
    "    with open(path,'w') as f:\n",
    "        if len(items) == 0:\n",
    "            return\n",
    "        headers = list(items[0].keys())\n",
    "        f.write(','.join(headers) + '\\n')\n",
    "        for item in items:\n",
    "            values = []\n",
    "            for header in headers:\n",
    "                values.append(str(item.get(header,\" \")))\n",
    "            f.write(\",\".join(values) + \"\\n\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c83a2a",
   "metadata": {},
   "source": [
    "## 9. Creating a single function to combine everything \n",
    "\n",
    "This final function does not take any arguments and return the required csv file. This function includes make the earlier defined function at one place and return the Top 100 Blogs on DigiStatement written to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72953645",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://digistatement.com/page/'\n",
    "def scrape_top_100_blogs():\n",
    "    '''Get the top 100 blogs for DigiStatement and write them to csv file'''\n",
    "    path = 'Top_100_Blogs.csv'\n",
    "    main = get_top100_blogs()\n",
    "    flat_list = flatten_list(main)\n",
    "    write_csv(flat_list,path)\n",
    "    print('Top 100 Blogs of DigiStatement written to file {}'.format(path))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e917ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 100 Blogs of DigiStatement written to file Top_100_Blogs.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Top_100_Blogs.csv'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_top_100_blogs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c328652b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In the current project the gaming website namely DigiStatement.com has been used for scrapping the articles on the 10 pages. Python as programming language and requests and BeautifulSoup libraries have been used to download the pages and then exploring and getting the relevant data from the website. Further the work has been in a csv file to be used for further processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadaa249",
   "metadata": {},
   "source": [
    "## Future works\n",
    "The current web scrappning project is a starting point for a bigger NLP project. The updates of which will be posted afterwards. In this project I started to look into some of the websites where I could get the data for news articles around the specific topic of 'Gaming'.\n",
    "\n",
    "Later the project will take a flight towards getting the information for user defined topics and scrape of the data accordingly from selective websites - at first the topics in Gaming and later in other areas as well.\n",
    "\n",
    "Another area to look into is how a particular topic is covered in the media and what is the general sense of the authors and which area of the spectrum of academia is the topic is tipping towards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a872e121",
   "metadata": {},
   "source": [
    "## Importing Jovian and commiting the work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e5549c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jovian in /opt/conda/lib/python3.9/site-packages (0.2.45)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from jovian) (6.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from jovian) (2.28.1)\n",
      "Requirement already satisfied: uuid in /opt/conda/lib/python3.9/site-packages (from jovian) (1.30)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from jovian) (8.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->jovian) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.9/site-packages (from requests->jovian) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->jovian) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->jovian) (1.26.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d6564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49147345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Updating notebook \"abhibhardwajabbb/abhishek-web-scraping-project-final\" on https://jovian.ai\u001b[0m\n",
      "[jovian] Committed successfully! https://jovian.ai/abhibhardwajabbb/abhishek-web-scraping-project-final\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ai/abhibhardwajabbb/abhishek-web-scraping-project-final'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d56176b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}